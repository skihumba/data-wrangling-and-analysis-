{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on wrangle_report.ipynb\n",
    "\n",
    "![WeRateDogs](./images/weratedogs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report outlines the processes followed when wrangling data from the twitter account [WeRateDogs](https://twitter.com/dog_rates). WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data wragling process was conducted in a jupyter notebook and involved 4 steps which are: \n",
    "1. Gathering the data\n",
    "\n",
    "2. Assessing the data\n",
    "\n",
    "3. Cleaning the data\n",
    "\n",
    "4. Storing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gathering the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used was obtained by using three methods. \n",
    "\n",
    "\n",
    "<div><img src=\"./images/download.png\" width=\"50\" height=\"35\" align=\"right\"/></div>\n",
    "\n",
    "- The first method involved directly downloading the WeRateDogs `twitter archive` data directly in the form of a csv file and then reading it directly into a Pandas dataframe as shown below.\n",
    "\n",
    "```Python \n",
    "df_tweet_archive_enhanced = pd.read_csv('./data/twitter-archive-enhanced.csv')\n",
    "```\n",
    "\n",
    "<div><img src=\"./images/requests.jpg\" width=\"50\" height=\"50\" align=\"right\"></div>\n",
    "\n",
    "- The second method involved using the `Requests` library to download the `tweet image` data. This was achieved by crating a function that would take the `url` of the file and then download the file to an already specified data folder as illustrated below. \n",
    "\n",
    "```Python\n",
    "def download_file(file_link):\n",
    "    response = requests.get(file_link)\n",
    "    with open(os.path.join(folder_name, file_link.split('/')[-1]), mode = 'wb') as file:\n",
    "        file.write(response.content)```\n",
    "        \n",
    "<div><img src=\"./images/twitterAPI.jpg\" width=\"150\" height=\"150\" align=\"right\"/></div>\n",
    "\n",
    "- The third method querrying the Twitter API ising the `tweepy`library to fetch additional tweet data such as `retweet_count` and `favourite_count`. Tweet ids were used to extract additional data fro each of the tweet in JSON. The data was stored in a file labelled `tweet_json.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Assessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step in the data wrangling process was assessing the data that had been gathered. The data was assessed both `visually` and `programmatically` in order to identify `quqlity` and `tidyness` issues\n",
    "\n",
    "\n",
    "- `visual` assessment was done by sampling the data and displaying it in the jupyter notebook. \n",
    "\n",
    "\n",
    "- `programmatic` assessment was done by using several Pandas functions such as: `.info()` to display information about the data, `.describe()` to describe the numeric columns in the data, `.isna()` to check for missing values and `.shape()` to get the size of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the assessment,  10 `quality` and 4 `tidyness` issues were identified:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the `quality` and `tidyness` issues identified in the assessin the data phase were cleaned in accordance to the rules of `tidy data`. \n",
    "\n",
    "The first step was to make copies of the data which was achieved by using the Pandas `.copy()` method.\n",
    "\n",
    "The data was then cleaned by using the `Define`, `Code` and `Test` approach that involved:\n",
    "- `defining` a cleaning task. \n",
    "-  writing `code` to perform the cleaning task.\n",
    "- `testing` the data with code to ensure the cleaning task is successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Storing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final stage in the wrangling process was storing the cleaned data into a single csv file. This was done by using the `.to_csv()` function of the Pandas Dataframe as shown below:\n",
    "\n",
    "```Python\n",
    "df_final.to_csv('./data/twitter_archive_master.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
